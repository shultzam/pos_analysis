{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read-in UCI sentiment-labelled Amazon data, count: 1000.\n"
     ]
    }
   ],
   "source": [
    "# Imports required for this cell.\n",
    "from os import path\n",
    "\n",
    "# Define some constant strings pointing to the UCI Amazon review data.\n",
    "DATA_DIR = '../data'\n",
    "UCI_DATA_DILE = 'uci_labelled_data/amazon_cells_labelled.txt'\n",
    "\n",
    "# Read in sentiment laballed sentences UCI's Amazon data file.\n",
    "uci_sentiment_data = []\n",
    "with open(path.join(DATA_DIR, UCI_DATA_DILE), encoding='utf-8') as fileObj:\n",
    "    uci_sentiment_data = [line.rstrip('\\n') for line in fileObj]\n",
    "    \n",
    "# Split each line into a tuple containing the text in the 0th index and the sentiment label in the 1st index.\n",
    "for index in range(0, len(uci_sentiment_data)):\n",
    "    split_sentence = uci_sentiment_data[index].split('\\t')\n",
    "    uci_sentiment_data[index] = (split_sentence[0], split_sentence[1])\n",
    "    \n",
    "print('Read-in UCI sentiment-labelled Amazon data, count: {}.'.format(len(uci_sentiment_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read-in UCSD Amazon data. Beauty count: 5269, Fashion count: 3176, Appliances count: 2272.\n"
     ]
    }
   ],
   "source": [
    "# Imports required for this cell.\n",
    "import json\n",
    "import gzip\n",
    "from os import path\n",
    "\n",
    "# Define some constant strings pointing to the UCSD Amazon review data.\n",
    "UCSD_DATA_DIR = path.join('../data', 'ucsd_amazon_data')\n",
    "UCSD_DATA_BEAUTY = 'All_Beauty_5.json.gz'\n",
    "UCSD_DATA_FASHION = 'AMAZON_FASHION_5.json.gz'\n",
    "UCSD_DATA_APPLIANCES = 'Appliances_5.json.gz'\n",
    "\n",
    "# Read in JSON data from UCSD's Amazon data files -- Beauty data.\n",
    "ucsd_beauty_data = []\n",
    "with gzip.open(path.join(UCSD_DATA_DIR, UCSD_DATA_BEAUTY)) as json_gzip:\n",
    "    for line in json_gzip:\n",
    "        ucsd_beauty_data.append(json.loads(line))\n",
    "\n",
    "# -- Beauty data.\n",
    "ucsd_fashion_data = []\n",
    "with gzip.open(path.join(UCSD_DATA_DIR, UCSD_DATA_FASHION)) as json_gzip:\n",
    "    for line in json_gzip:\n",
    "        ucsd_fashion_data.append(json.loads(line))\n",
    "\n",
    "# -- Appliance data.\n",
    "ucsd_appliances_data = []\n",
    "with gzip.open(path.join(UCSD_DATA_DIR, UCSD_DATA_APPLIANCES)) as json_gzip:\n",
    "    for line in json_gzip:\n",
    "        ucsd_appliances_data.append(json.loads(line))\n",
    "        \n",
    "print('Read-in UCSD Amazon data. Beauty count: {}, Fashion count: {}, Appliances count: {}.'.format(len(ucsd_beauty_data), len(ucsd_fashion_data), len(ucsd_appliances_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the brown corpus.\n",
    "from nltk.corpus import brown as brown_corpus\n",
    "# Collections imports related to this cell.\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Process the Brown corpus, storing it into a defaultDict. This allows for easy finding of a specific POS tag.\n",
    "# Note: These keys are case sensitive so the key 'movie' differs from 'Movie'.\n",
    "word_tags = defaultdict(Counter)\n",
    "for word, pos in brown_corpus.tagged_words(tagset='universal'):\n",
    "    word_tags[word][pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_text: I like this as a vent as well as something that will keep house warmer in winter  I sanded it and then painted it the same color as the house  Looks great\n",
      "   subject_map: defaultdict(<class 'collections.Counter'>, {'I': 5, 'like': 0, 'this': 0, 'as': 0, 'a': 1, 'vent': 7, 'well': 17, 'something': 420, 'that': 0, 'will': 99, 'keep': 3, 'house': 391, 'warmer': 0, 'in': 1, 'winter': 76, 'it': 0, 'and': 0, 'then': 0, 'painted': 0, 'the': 0, 'same': 0, 'color': 132, 'Looks': 1, 'great': 0})\n",
      "   max word: winter\n",
      "\n",
      "\n",
      "\n",
      "review_text: I purchasaed a new dryer and did not want to reuse the cord from my old unit This unit installed in a pretty straight forward manor Quality was as expected No Complaints\n",
      "   subject_map: defaultdict(<class 'collections.Counter'>, {'I': 5, 'a': 1, 'new': 0, 'dryer': 3, 'and': 0, 'did': 0, 'not': 0, 'want': 9, 'to': 1, 'the': 0, 'cord': 6, 'from': 0, 'my': 0, 'old': 0, 'unit': 94, 'This': 0, 'installed': 0, 'in': 1, 'pretty': 0, 'straight': 0, 'forward': 0, 'manor': 3, 'Quality': 2, 'was': 0, 'as': 0, 'expected': 0, 'No': 0})\n",
      "   max word: was\n",
      "\n",
      "\n",
      "\n",
      "review_text: works great we loved ours till we didnt  these do not last so buy the warranty as you WILL NEED IT\n",
      "   subject_map: defaultdict(<class 'collections.Counter'>, {'works': 88, 'great': 0, 'we': 0, 'loved': 0, 'ours': 0, 'till': 0, 'these': 0, 'do': 0, 'not': 0, 'last': 8, 'so': 0, 'buy': 1, 'the': 0, 'warranty': 1, 'as': 0, 'you': 0})\n",
      "   max word: you\n",
      "\n",
      "\n",
      "\n",
      "review_text: Luved it for the few months it worked  great little bullet shaped ice cubes It was a gift for my sister who never opened the box  The next summer during a heat wave I asked for my unused gift back ha and was in heaven for a few months  the next summer after a few weeks the unit gave outcalled new air and it was over the one year mark so no warranty  so my advice is do not buy without  extended warranty\n",
      "   subject_map: defaultdict(<class 'collections.Counter'>, {'it': 0, 'for': 3, 'the': 0, 'few': 0, 'months': 188, 'worked': 0, 'great': 0, 'little': 0, 'bullet': 26, 'shaped': 0, 'ice': 43, 'cubes': 4, 'It': 0, 'was': 0, 'a': 1, 'gift': 32, 'my': 0, 'sister': 34, 'who': 0, 'never': 0, 'opened': 0, 'box': 64, 'The': 0, 'next': 0, 'summer': 132, 'during': 0, 'heat': 86, 'wave': 44, 'I': 5, 'asked': 0, 'unused': 0, 'back': 176, 'ha': 0, 'and': 0, 'in': 1, 'heaven': 23, 'after': 0, 'weeks': 139, 'unit': 94, 'gave': 0, 'new': 0, 'air': 218, 'over': 0, 'one': 482, 'year': 647, 'mark': 36, 'so': 0, 'no': 0, 'warranty': 1, 'advice': 50, 'is': 0, 'do': 0, 'not': 0, 'buy': 1, 'without': 0, 'extended': 0})\n",
      "   max word: year\n",
      "\n",
      "\n",
      "\n",
      "review_text: Did the job for fixing our Maytag dryer\n",
      "   subject_map: defaultdict(<class 'collections.Counter'>, {'Did': 0, 'the': 0, 'job': 236, 'for': 3, 'fixing': 2, 'our': 0, 'dryer': 3})\n",
      "   max word: the\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports related to this cell.\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Test on a random list of the UCSD Amazon data.\n",
    "punct_table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "for review_obj in ucsd_appliances_data[0:15]:\n",
    "    # Save off the review text with no punctuation.\n",
    "    review_text = review_obj['reviewText'].translate(punct_table)\n",
    "    \n",
    "    # If the review is less than 8 words or more than 200 words, skip it. \n",
    "    # This is an attempt to skip over reviews difficult to analyze.\n",
    "    review_word_list = review_text.split(' ')\n",
    "    if len(review_word_list) < 8 or len(review_word_list) > 200:\n",
    "        continue\n",
    "    \n",
    "    # Create a map tracking the NN tag count for each word in the review.\n",
    "    print('review_text: {}'.format(review_text))\n",
    "    subject_map = defaultdict(Counter)\n",
    "    for word in review_word_list:\n",
    "        # Collect the POS tag counts for this word.\n",
    "        word_tag_counts = word_tags[word]\n",
    "        #print('   word: {} --> POS tag(s): {}'.format(word, word_tag_counts))\n",
    "        \n",
    "        # If this word has no POS tags, no more work is needed for this word.\n",
    "        if 0 == len(word_tag_counts):\n",
    "            #print('      No POS tags present for this word, skipping.')\n",
    "            continue\n",
    "        \n",
    "        # Collect some data and update the subject_map.\n",
    "        word_tag_sum = sum(word_tag_counts.values())\n",
    "        percentage_noun = float(word_tag_counts['NOUN'] / word_tag_sum)\n",
    "        subject_map[word] = word_tag_counts['NOUN']\n",
    "        \n",
    "        # Print some information about this word.\n",
    "        #print('      NOUN count: {}, percentage: {}'.format(word_tag_counts['NOUN'], percentage_noun))\n",
    "        \n",
    "    # Do some initial guesswork on what the subject of the review may be.\n",
    "    print('   subject_map: {}'.format(subject_map))\n",
    "    print('   max word: {}\\n\\n\\n'.format(max(subject_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectives common in positive reviews: defaultdict(<class 'int'>, {'very': 69, 'great': 62, 'good': 53, 'best': 19, 'happy': 13, 'easy': 12, 'better': 11, 'new': 10, 'clear': 7, 'much': 7, 'fine': 6, 'other': 6, 'long': 6, 'little': 5, 'original': 4, 'several': 4, 'most': 4, 'free': 4, 'small': 4, 'sure': 3, 'able': 3, 'many': 3, 'simple': 3, 'own': 3, 'beautiful': 2, 'whole': 2, 'basic': 1, 'available': 1, 'entire': 1, 'open': 1})\n",
      "\n",
      "Adjectives common in negative reviews: defaultdict(<class 'int'>, {'only': 17, 'first': 13, 'bad': 11, 'same': 11, 'enough': 10, 'poor': 9, 'few': 9, 'more': 8, 'right': 6, 'difficult': 6, 'last': 5, 'black': 4, 'old': 4, 'real': 4, 'different': 3, 'big': 3, 'dead': 3, 'important': 3, 'hard': 3, 'particular': 2, 'such': 2, 'strong': 2, 'wrong': 2, 'greater': 1, 'white': 1, 'due': 1, 'industrial': 1, 'ready': 1, 'certain': 1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell related imports.\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "\n",
    "# Get the most common adjectives in the brown corpus based on adjective tag counts.\n",
    "adjectives_dist = FreqDist(word for (word, tag) in brown_corpus.tagged_words(tagset=\"universal\") if tag == 'ADJ')\n",
    "common_adjectives = adjectives_dist.most_common(160)\n",
    "common_adjectives_no_count = [pair[0] for pair in common_adjectives]\n",
    "\n",
    "# Create a frequency distribution of words occurring in positive/negative reviews.\n",
    "negative_review_adjs = defaultdict(int)\n",
    "positive_review_adjs = defaultdict(int)\n",
    "for review_text, sentiment in uci_sentiment_data:\n",
    "    # Split the review into a list of words.\n",
    "    review_word_list = review_text.lower().split(' ')\n",
    "    \n",
    "    # Negative sentiment reviews.\n",
    "    if sentiment == '0':\n",
    "        for word in review_word_list:\n",
    "            if word in common_adjectives_no_count:\n",
    "                negative_review_adjs[word] += 1\n",
    "    # Positive sentiment reviews.\n",
    "    elif sentiment == '1':\n",
    "        for word in review_word_list:\n",
    "            if word in common_adjectives_no_count:\n",
    "                positive_review_adjs[word] += 1\n",
    "    # Uncaught issue.\n",
    "    else:\n",
    "        print('WARNING: Unrecognized sentiment label: {}'.format(sentiment))\n",
    "        continue\n",
    "        \n",
    "# Create sorted lists showing the most common words for both review types.\n",
    "negative_review_adjs_list = FreqDist(negative_review_adjs).most_common(100)\n",
    "positive_review_adjs_list = FreqDist(positive_review_adjs).most_common(100)\n",
    "\n",
    "# Remove words that are common in both lists.\n",
    "negative_review_adjs_no_count = [pair[0] for pair in negative_review_adjs_list]\n",
    "positive_review_adjs_no_count = [pair[0] for pair in positive_review_adjs_list]\n",
    "joint_adjs_list = set(negative_review_adjs_no_count) & set(positive_review_adjs_no_count)\n",
    "\n",
    "# Convert these lists to dictionary for ease of access.\n",
    "negative_review_adjs = defaultdict(int)\n",
    "positive_review_adjs = defaultdict(int)\n",
    "for word, count in negative_review_adjs_list:\n",
    "    negative_review_adjs[word] = count\n",
    "for word, count in positive_review_adjs_list:\n",
    "    positive_review_adjs[word] = count\n",
    "\n",
    "# Remove the joint adjectives list from whichever list features this word less often.\n",
    "for joint_adj in joint_adjs_list:\n",
    "    # If the counts in both positive and negative are close, remove from both.\n",
    "    if negative_review_adjs[joint_adj] > positive_review_adjs[joint_adj]:\n",
    "        positive_review_adjs.pop(joint_adj)\n",
    "    elif negative_review_adjs[joint_adj] < positive_review_adjs[joint_adj]:\n",
    "        negative_review_adjs.pop(joint_adj)\n",
    "    else:\n",
    "        positive_review_adjs.pop(joint_adj)\n",
    "        negative_review_adjs.pop(joint_adj)\n",
    "\n",
    "# Print out the common adjectives for both reivew sentiments.\n",
    "print('Adjectives common in positive reviews: {}\\n'.format(positive_review_adjs))\n",
    "print('Adjectives common in negative reviews: {}\\n'.format(negative_review_adjs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "Predicted    0    1  No data  No prediction\n",
      "Actual                                     \n",
      "0          106   68      310             16\n",
      "1           15  245      229             11\n",
      "\n",
      "Missclassification rate (N/d not counted): 0.239\n",
      "Missclassification rate (N/d counted)    : 0.649\n",
      "No data rate                             : 0.539\n",
      "NOTE: N/d --> No data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the sentiment-labelled data and predict sentiment.\n",
    "prediction_list = []\n",
    "sentiment_list = []\n",
    "for review_text, sentiment in uci_sentiment_data:\n",
    "    # Track sentiment prediction.\n",
    "    prediction = 'No data'\n",
    "    \n",
    "    # Loop through each word in the review_text.\n",
    "    review_word_list = review_text.lower().split(' ')\n",
    "    for word in review_word_list:\n",
    "        if word in negative_review_adjs:\n",
    "            if isinstance(prediction, str):\n",
    "                prediction = 0\n",
    "            prediction -= 1\n",
    "        elif word in positive_review_adjs:\n",
    "            if isinstance(prediction, str):\n",
    "                prediction = 0\n",
    "            prediction += 1\n",
    "    \n",
    "    # Convert the prediction scheme to match the sentiment 0/1 labels.\n",
    "    if isinstance(prediction, str):\n",
    "        pass\n",
    "    elif prediction > 0:\n",
    "        prediction = '1'\n",
    "    elif prediction < 0:\n",
    "        prediction = '0'\n",
    "    elif prediction == 0:\n",
    "        prediction = 'No prediction'\n",
    "        \n",
    "    # Save off the prediction and sentiment into lists.\n",
    "    prediction_list.append(prediction)\n",
    "    sentiment_list.append(sentiment)\n",
    "\n",
    "# Print the confusion matrix.\n",
    "sentiment_list = pd.Series(sentiment_list, name='Actual')\n",
    "prediction_list = pd.Series(prediction_list, name='Predicted')\n",
    "pd_confusion = pd.crosstab(sentiment_list, prediction_list)\n",
    "print('Confusion matrix: \\n{}\\n'.format(pd_confusion))\n",
    "\n",
    "# Print the misclassification rate (0/1 loss) and cannot classify rate.\n",
    "missclassification_count = 0\n",
    "unclassification_count = 0\n",
    "for index in range(len(prediction_list)):\n",
    "    if prediction_list[index] == 'No data':\n",
    "        unclassification_count += 1\n",
    "    elif prediction_list[index] != sentiment_list[index]:\n",
    "        missclassification_count += 1\n",
    "\n",
    "missclassified_rate = float(1/(len(uci_sentiment_data) - unclassification_count) * missclassification_count)\n",
    "missclassified_rate_with_na = float(1/len(uci_sentiment_data) * (missclassification_count + unclassification_count))\n",
    "unclassified_rate = float(1/len(uci_sentiment_data) * unclassification_count)\n",
    "\n",
    "print('Missclassification rate (N/d not counted): {:.3f}'.format(missclassified_rate))\n",
    "print('Missclassification rate (N/d counted)    : {:.3f}'.format(missclassified_rate_with_na))\n",
    "print('No data rate                             : {:.3f}'.format(unclassified_rate))\n",
    "print('NOTE: N/d --> No data')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 510 ucsd_appliances_data.\n",
      "Confusion matrix: \n",
      "Predicted   0    1  No data  No prediction\n",
      "Actual                                    \n",
      "0          84   43       73             10\n",
      "1          14  155      121             10\n",
      "\n",
      "Missclassification rate (N/d not counted): 0.244\n",
      "Missclassification rate (N/d counted)    : 0.531\n",
      "No data rate                             : 0.380\n",
      "NOTE: N/d --> No data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the appliances Amazon review dataset. First, create a list of sentiment labelled reviews.\n",
    "processed_data = list()\n",
    "neg_count = 0\n",
    "pos_count = 0\n",
    "for review in ucsd_fashion_data:\n",
    "    # Skip over incomplete reviews.\n",
    "    if 'reviewText' not in review:\n",
    "        continue\n",
    "    \n",
    "    # Once we have an evenly distributed review list we can exit the processing.\n",
    "    if neg_count == 300 and pos_count == 300:\n",
    "        break\n",
    "    \n",
    "    # Process the review, saving into a (review_text, sentiment 0/1) tuple.\n",
    "    sentiment_label = -1\n",
    "    if (review['overall'] == 1.0 or review['overall'] == 2.0) and neg_count < 300:\n",
    "        sentiment_label = '0'\n",
    "        neg_count += 1\n",
    "    elif (review['overall'] == 4.0 or review['overall'] == 5.0) and pos_count < 300:\n",
    "        sentiment_label = '1'\n",
    "        pos_count += 1\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Append this review to our list.\n",
    "    processed_data.append((review['reviewText'].lower(), sentiment_label))\n",
    "\n",
    "print('Finished processing {} ucsd_appliances_data.'.format(len(processed_data)))\n",
    "    \n",
    "# Analyze the Amazon appliance data and predict sentiment.\n",
    "prediction_list = []\n",
    "sentiment_list = []\n",
    "for review_text, sentiment in processed_data:\n",
    "    # Track sentiment prediction.\n",
    "    prediction = 'No data'\n",
    "    \n",
    "    # Loop through each word in the review_text.\n",
    "    review_word_list = review_text.lower().split(' ')\n",
    "    for word in review_word_list:\n",
    "        if word in negative_review_adjs:\n",
    "            if isinstance(prediction, str):\n",
    "                prediction = 0\n",
    "            prediction -= 1\n",
    "        elif word in positive_review_adjs:\n",
    "            if isinstance(prediction, str):\n",
    "                prediction = 0\n",
    "            prediction += 1\n",
    "    \n",
    "    # Convert the prediction scheme to match the sentiment 0/1 labels.\n",
    "    if isinstance(prediction, str):\n",
    "        pass\n",
    "    elif prediction > 0:\n",
    "        prediction = '1'\n",
    "    elif prediction < 0:\n",
    "        prediction = '0'\n",
    "    elif prediction == 0:\n",
    "        prediction = 'No prediction'\n",
    "        \n",
    "    # Save off the prediction and sentiment into lists.\n",
    "    prediction_list.append(prediction)\n",
    "    sentiment_list.append(sentiment)\n",
    "    \n",
    "# Print the confusion matrix.\n",
    "sentiment_list = pd.Series(sentiment_list, name='Actual')\n",
    "prediction_list = pd.Series(prediction_list, name='Predicted')\n",
    "pd_confusion = pd.crosstab(sentiment_list, prediction_list)\n",
    "print('Confusion matrix: \\n{}\\n'.format(pd_confusion))\n",
    "\n",
    "# Print the misclassification rate (0/1 loss) and cannot classify rate.\n",
    "missclassification_count = 0\n",
    "unclassification_count = 0\n",
    "for index in range(len(prediction_list)):\n",
    "    if prediction_list[index] == 'No data':\n",
    "        unclassification_count += 1\n",
    "    elif prediction_list[index] != sentiment_list[index]:\n",
    "        missclassification_count += 1\n",
    "\n",
    "missclassified_rate = float(1/(len(processed_data) - unclassification_count) * missclassification_count)\n",
    "missclassified_rate_with_na = float(1/len(processed_data) * (missclassification_count + unclassification_count))\n",
    "unclassified_rate = float(1/len(processed_data) * unclassification_count)\n",
    "\n",
    "print('Missclassification rate (N/d not counted): {:.3f}'.format(missclassified_rate))\n",
    "print('Missclassification rate (N/d counted)    : {:.3f}'.format(missclassified_rate_with_na))\n",
    "print('No data rate                             : {:.3f}'.format(unclassified_rate))\n",
    "print('NOTE: N/d --> No data')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
